{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b022e2b-2924-4594-9c82-f32c9d3bc6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import numpy as np\n",
    "from transformers import Blip2ForConditionalGeneration, Blip2Processor\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# ====================================================\n",
    "# Constants and Configuration\n",
    "# ====================================================\n",
    "\n",
    "# Model and dataset configuration\n",
    "MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"  # BLIP-2 model to load\n",
    "DATASET = \"coco_captions\"  # Dataset for calibration and evaluation\n",
    "DATASET_PATH = \"../data/coco/val2017\"       # Path to the COCO dataset directory\n",
    "ANNOTATIONS_FILE = \"../data/coco/annotations/captions_val2017.json\"  # Path to the COCO annotations file\n",
    "\n",
    "# Quantization parameters\n",
    "SEED = 0  # Random seed for reproducibility\n",
    "NUM_SAMPLES = 16  # Number of calibration samples\n",
    "PERCENT_DAMPENING = 0.01  # Percentage for dampening during quantization\n",
    "BITS = 4  # Number of bits for quantization\n",
    "GROUP_SIZE = -1  # Group size for quantization\n",
    "USE_SYMMETRIC = True  # Use symmetric quantization\n",
    "USE_ACT_ORDER = False  # Use activation order during quantization\n",
    "USE_STATIC_GROUPS = False  # Use static groups during quantization\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Debugging flag\n",
    "DEBUG = False  # Set to True for debugging output\n",
    "\n",
    "# Disable TensorFloat32 for matmul and cuDNN to ensure deterministic results\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# ====================================================\n",
    "# Quantization Functions and Classes\n",
    "# ====================================================\n",
    "\n",
    "def quantize(x, scale, zero, maxq):\n",
    "    \"\"\"\n",
    "    Quantize the input tensor x using the provided scale and zero point.\n",
    "    If maxq < 0, use a special case quantization.\n",
    "    \"\"\"\n",
    "    if maxq < 0:\n",
    "        # Special case for ternary quantization\n",
    "        return (x > scale / 2).float() * scale + (x < zero / 2).float() * zero\n",
    "    q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n",
    "    return scale * (q - zero)\n",
    "\n",
    "class Quantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantizer class to handle quantization parameters and operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, shape=1):\n",
    "        super(Quantizer, self).__init__()\n",
    "        self.register_buffer('maxq', torch.tensor(0))  # Maximum quantization level\n",
    "        self.register_buffer('scale', torch.zeros(shape))  # Scale for quantization\n",
    "        self.register_buffer('zero', torch.zeros(shape))  # Zero point for quantization\n",
    "\n",
    "    def configure(self, bits, perchannel=False, sym=True, mse=False, norm=2.4,\n",
    "                  grid=100, maxshrink=0.8, trits=False):\n",
    "        \"\"\"\n",
    "        Configure the quantizer with the specified parameters.\n",
    "        \"\"\"\n",
    "        self.maxq = torch.tensor(2 ** bits - 1)\n",
    "        self.perchannel = perchannel  # Whether to quantize per channel\n",
    "        self.sym = sym  # Symmetric quantization\n",
    "        self.mse = mse  # Use MSE for scale and zero point calculation\n",
    "        self.norm = norm  # Norm for error calculation\n",
    "        self.grid = grid  # Grid size for scale search\n",
    "        self.maxshrink = maxshrink  # Maximum shrinkage for scale search\n",
    "        if trits:\n",
    "            self.maxq = torch.tensor(-1)  # Special value for ternary quantization\n",
    "\n",
    "    def find_params(self, x, weight=False):\n",
    "        \"\"\"\n",
    "        Find the scale and zero point parameters for quantization based on input tensor x.\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        self.maxq = self.maxq.to(device)\n",
    "\n",
    "        shape = x.shape\n",
    "        if self.perchannel:\n",
    "            # Per-channel quantization\n",
    "            if weight:\n",
    "                x = x.flatten(1)\n",
    "            else:\n",
    "                if len(shape) == 4:\n",
    "                    x = x.permute(1, 0, 2, 3)\n",
    "                    x = x.flatten(1)\n",
    "                elif len(shape) == 3:\n",
    "                    x = x.reshape(-1, shape[-1]).t()\n",
    "                elif len(shape) == 2:\n",
    "                    x = x.t()\n",
    "        else:\n",
    "            # Global quantization\n",
    "            x = x.flatten().unsqueeze(0)\n",
    "\n",
    "        tmp = torch.zeros(x.shape[0], device=device)\n",
    "        xmin = torch.minimum(x.min(dim=1)[0], tmp)\n",
    "        xmax = torch.maximum(x.max(dim=1)[0], tmp)\n",
    "\n",
    "        if self.sym:\n",
    "            xmax = torch.maximum(torch.abs(xmin), xmax)\n",
    "            negative_mask = xmin < 0\n",
    "            if torch.any(negative_mask):\n",
    "                xmin[negative_mask] = -xmax[negative_mask]\n",
    "        zero_mask = (xmin == 0) & (xmax == 0)\n",
    "        xmin[zero_mask] = -1\n",
    "        xmax[zero_mask] = 1\n",
    "\n",
    "        if self.maxq < 0:\n",
    "            # Special case for ternary quantization\n",
    "            self.scale = xmax\n",
    "            self.zero = xmin\n",
    "        else:\n",
    "            self.scale = (xmax - xmin) / self.maxq\n",
    "            if self.sym:\n",
    "                self.zero = torch.full_like(self.scale, (self.maxq + 1) / 2)\n",
    "            else:\n",
    "                self.zero = torch.round(-xmin / self.scale)\n",
    "\n",
    "        if self.mse:\n",
    "            # Use Mean Squared Error to find optimal scale and zero point\n",
    "            best_error = torch.full([x.shape[0]], float('inf'), device=device)\n",
    "            for i in range(int(self.maxshrink * self.grid)):\n",
    "                p = 1 - i / self.grid\n",
    "                xmin1 = p * xmin\n",
    "                xmax1 = p * xmax\n",
    "                scale1 = (xmax1 - xmin1) / self.maxq\n",
    "                zero1 = torch.round(-xmin1 / scale1) if not self.sym else self.zero\n",
    "                q = quantize(x, scale1.unsqueeze(1), zero1.unsqueeze(1), self.maxq)\n",
    "                error = ((q - x).abs().pow(self.norm)).sum(dim=1)\n",
    "                better_error_mask = error < best_error\n",
    "                if torch.any(better_error_mask):\n",
    "                    best_error[better_error_mask] = error[better_error_mask]\n",
    "                    self.scale[better_error_mask] = scale1[better_error_mask]\n",
    "                    self.zero[better_error_mask] = zero1[better_error_mask]\n",
    "\n",
    "        if not self.perchannel:\n",
    "            repeat_times = shape[0] if weight else shape[1] if len(shape) != 3 else shape[2]\n",
    "            self.scale = self.scale.repeat(repeat_times)\n",
    "            self.zero = self.zero.repeat(repeat_times)\n",
    "\n",
    "        if weight:\n",
    "            # Reshape for weight tensors\n",
    "            new_shape = [-1] + [1] * (len(shape) - 1)\n",
    "            self.scale = self.scale.reshape(new_shape)\n",
    "            self.zero = self.zero.reshape(new_shape)\n",
    "            return\n",
    "\n",
    "        # Reshape for activation tensors\n",
    "        if len(shape) == 4:\n",
    "            self.scale = self.scale.reshape(1, -1, 1, 1)\n",
    "            self.zero = self.zero.reshape(1, -1, 1, 1)\n",
    "        elif len(shape) == 3:\n",
    "            self.scale = self.scale.reshape(1, 1, -1)\n",
    "            self.zero = self.zero.reshape(1, 1, -1)\n",
    "        elif len(shape) == 2:\n",
    "            self.scale = self.scale.unsqueeze(0)\n",
    "            self.zero = self.zero.unsqueeze(0)\n",
    "\n",
    "    def quantize(self, x):\n",
    "        \"\"\"\n",
    "        Quantize the input tensor x using the stored scale and zero point.\n",
    "        \"\"\"\n",
    "        if self.ready():\n",
    "            return quantize(x, self.scale, self.zero, self.maxq)\n",
    "        return x\n",
    "\n",
    "    def enabled(self):\n",
    "        \"\"\"\n",
    "        Check if quantization is enabled (maxq > 0).\n",
    "        \"\"\"\n",
    "        return self.maxq > 0\n",
    "\n",
    "    def ready(self):\n",
    "        \"\"\"\n",
    "        Check if the quantizer is ready (scale is non-zero).\n",
    "        \"\"\"\n",
    "        return torch.all(self.scale != 0)\n",
    "\n",
    "# ====================================================\n",
    "# GPTQ Quantization Class\n",
    "# ====================================================\n",
    "\n",
    "class GPTQ:\n",
    "    \"\"\"\n",
    "    GPTQ class to perform quantization of a given model layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.device = self.layer.weight.device\n",
    "        W = layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.rows, self.columns = W.shape\n",
    "        self.H = torch.zeros((self.columns, self.columns), device=self.device)\n",
    "        self.nsamples = 0  # Number of samples collected\n",
    "        self.quantizer = Quantizer()\n",
    "\n",
    "    def add_batch(self, inp, out):\n",
    "        \"\"\"\n",
    "        Add a batch of input and output data to compute Hessian approximation.\n",
    "        \"\"\"\n",
    "        if DEBUG:\n",
    "            self.inp1 = inp\n",
    "            self.out1 = out\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        batch_size = inp.shape[0]\n",
    "        if isinstance(self.layer, (nn.Linear, transformers.Conv1D)):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape(-1, inp.shape[-1])\n",
    "            inp = inp.t()\n",
    "        elif isinstance(self.layer, nn.Conv2d):\n",
    "            unfold = nn.Unfold(\n",
    "                self.layer.kernel_size,\n",
    "                dilation=self.layer.dilation,\n",
    "                padding=self.layer.padding,\n",
    "                stride=self.layer.stride\n",
    "            )\n",
    "            inp = unfold(inp)\n",
    "            inp = inp.permute(1, 0, 2)\n",
    "            inp = inp.flatten(1)\n",
    "        # Update Hessian approximation\n",
    "        self.H *= self.nsamples / (self.nsamples + batch_size)\n",
    "        self.nsamples += batch_size\n",
    "        inp = math.sqrt(2 / self.nsamples) * inp.float()\n",
    "        self.H += inp.matmul(inp.t())\n",
    "\n",
    "    def fasterquant(self, blocksize=128, percdamp=0.01, groupsize=-1,\n",
    "                    actorder=False, static_groups=False):\n",
    "        \"\"\"\n",
    "        Perform the quantization using the collected data and Hessian approximation.\n",
    "        \"\"\"\n",
    "        W = self.layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        W = W.float()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not self.quantizer.ready():\n",
    "            self.quantizer.find_params(W, weight=True)\n",
    "\n",
    "        H = self.H\n",
    "        del self.H\n",
    "        dead_mask = torch.diag(H) == 0\n",
    "        H[dead_mask, dead_mask] = 1\n",
    "        W[:, dead_mask] = 0\n",
    "\n",
    "        if static_groups:\n",
    "            import copy\n",
    "            groups = []\n",
    "            for i in range(0, self.columns, groupsize):\n",
    "                quantizer = copy.deepcopy(self.quantizer)\n",
    "                quantizer.find_params(W[:, i:i+groupsize], weight=True)\n",
    "                groups.append(quantizer)\n",
    "\n",
    "        if actorder:\n",
    "            perm = torch.argsort(torch.diag(H), descending=True)\n",
    "            W = W[:, perm]\n",
    "            H = H[perm][:, perm]\n",
    "            inv_perm = torch.argsort(perm)\n",
    "\n",
    "        losses = torch.zeros_like(W)\n",
    "        Q = torch.zeros_like(W)\n",
    "\n",
    "        damp = percdamp * torch.mean(torch.diag(H))\n",
    "        diag_indices = torch.arange(self.columns, device=self.device)\n",
    "        H[diag_indices, diag_indices] += damp\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        H_inv = H\n",
    "\n",
    "        for i1 in range(0, self.columns, blocksize):\n",
    "            i2 = min(i1 + blocksize, self.columns)\n",
    "            count = i2 - i1\n",
    "\n",
    "            W1 = W[:, i1:i2].clone()\n",
    "            Q1 = torch.zeros_like(W1)\n",
    "            Err1 = torch.zeros_like(W1)\n",
    "            Losses1 = torch.zeros_like(W1)\n",
    "            H_inv1 = H_inv[i1:i2, i1:i2]\n",
    "\n",
    "            for i in range(count):\n",
    "                w = W1[:, i]\n",
    "                d = H_inv1[i, i]\n",
    "\n",
    "                if groupsize != -1:\n",
    "                    if not static_groups:\n",
    "                        if (i1 + i) % groupsize == 0:\n",
    "                            self.quantizer.find_params(W[:, (i1 + i):(i1 + i + groupsize)], weight=True)\n",
    "                    else:\n",
    "                        idx = i1 + i\n",
    "                        if actorder:\n",
    "                            idx = perm[idx]\n",
    "                        self.quantizer = groups[idx // groupsize]\n",
    "\n",
    "                q = quantize(w.unsqueeze(1), self.quantizer.scale, self.quantizer.zero, self.quantizer.maxq).flatten()\n",
    "                Q1[:, i] = q\n",
    "                Losses1[:, i] = (w - q).pow(2) / d.pow(2) / 2\n",
    "\n",
    "                err1 = (w - q) / d\n",
    "                W1[:, i:] -= err1.unsqueeze(1).matmul(H_inv1[i, i:].unsqueeze(0))\n",
    "                Err1[:, i] = err1\n",
    "\n",
    "            Q[:, i1:i2] = Q1\n",
    "            losses[:, i1:i2] = Losses1\n",
    "\n",
    "            W[:, i2:] -= Err1.matmul(H_inv[i1:i2, i2:])\n",
    "\n",
    "            if DEBUG:\n",
    "                self.layer.weight.data[:, :i2] = Q[:, :i2]\n",
    "                self.layer.weight.data[:, i2:] = W[:, i2:]\n",
    "                print(torch.sum((self.layer(self.inp1) - self.out1).pow(2)))\n",
    "                print(torch.sum(losses))\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Time for quantization: {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"Total quantization error: {torch.sum(losses).item()}\")\n",
    "\n",
    "        if actorder:\n",
    "            Q = Q[:, inv_perm]\n",
    "\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            Q = Q.t()\n",
    "        self.layer.weight.data = Q.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)\n",
    "        if DEBUG:\n",
    "            print(torch.sum((self.layer(self.inp1) - self.out1).pow(2)))\n",
    "\n",
    "    def free(self):\n",
    "        \"\"\"\n",
    "        Free up memory by deleting large variables.\n",
    "        \"\"\"\n",
    "        if DEBUG:\n",
    "            self.inp1 = None\n",
    "            self.out1 = None\n",
    "        self.H = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ====================================================\n",
    "# Data Loader Functions\n",
    "# ====================================================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_coco_dataset(nsamples, seed, dataset_path, annotations_file):\n",
    "    \"\"\"\n",
    "    Load the COCO dataset and prepare calibration and test data loaders.\n",
    "    \"\"\"\n",
    "    from pycocotools.coco import COCO\n",
    "\n",
    "    # Initialize COCO API for caption annotations\n",
    "    coco = COCO(annotations_file)\n",
    "\n",
    "    # Get all image IDs\n",
    "    img_ids = list(coco.imgs.keys())\n",
    "\n",
    "    # Set random seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Shuffle and select samples\n",
    "    random.shuffle(img_ids)\n",
    "    selected_img_ids = img_ids[:nsamples]\n",
    "    test_img_ids = img_ids[nsamples:nsamples + 100]  # Use next 100 images for testing\n",
    "\n",
    "    processor = Blip2Processor.from_pretrained(MODEL_NAME)\n",
    "    trainloader = []\n",
    "\n",
    "    # Prepare the calibration data loader\n",
    "    for img_id in selected_img_ids:\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(dataset_path, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        trainloader.append(inputs)\n",
    "\n",
    "    # Prepare the test data loader\n",
    "    testloader = []\n",
    "    for img_id in test_img_ids:\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(dataset_path, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # Get all captions for the image\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        captions = [ann['caption'] for ann in anns]\n",
    "        # Use the first caption as reference\n",
    "        reference_caption = captions[0] if captions else \"\"\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        testloader.append((inputs, reference_caption))\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "def get_loaders(nsamples, seed, dataset_path, annotations_file):\n",
    "    \"\"\"\n",
    "    Get the calibration and test data loaders for the COCO dataset.\n",
    "    \"\"\"\n",
    "    return get_coco_dataset(nsamples, seed, dataset_path, annotations_file)\n",
    "\n",
    "# ====================================================\n",
    "# Model Utility Functions\n",
    "# ====================================================\n",
    "\n",
    "def find_layers(module, layers=[nn.Conv2d, nn.Linear, transformers.Conv1D], name=''):\n",
    "    \"\"\"\n",
    "    Recursively find all layers of specified types in a model.\n",
    "    Returns a dictionary mapping layer names to layers.\n",
    "    \"\"\"\n",
    "    if type(module) in layers:\n",
    "        return {name: module}\n",
    "    res = {}\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_res = find_layers(\n",
    "            child_module, layers=layers, name=name + '.' + child_name if name else child_name\n",
    "        )\n",
    "        res.update(child_res)\n",
    "    return res\n",
    "\n",
    "def get_blip2(model_name):\n",
    "    \"\"\"\n",
    "    Load and prepare the BLIP-2 model for quantization.\n",
    "    \"\"\"\n",
    "    # Disable weight initialization to speed up model loading\n",
    "    def skip_init(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    torch.nn.init.kaiming_uniform_ = skip_init\n",
    "    torch.nn.init.uniform_ = skip_init\n",
    "    torch.nn.init.normal_ = skip_init\n",
    "\n",
    "    # Load the model\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(model_name, torch_dtype='auto')\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def blip2_sequential(model, dataloader):\n",
    "    print(\"Starting quantization...\")\n",
    "\n",
    "    use_cache = getattr(model.config, 'use_cache', False)\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    model.eval()\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "\n",
    "    print(\"Collecting calibration data...\")\n",
    "\n",
    "    layers = find_layers(model)\n",
    "    layers_to_quantize = {name: layer for name, layer in layers.items() if isinstance(layer, nn.Linear)}\n",
    "\n",
    "    gptq_layers = {}\n",
    "    for name, layer in layers_to_quantize.items():\n",
    "        gptq = GPTQ(layer)\n",
    "        gptq.quantizer.configure(\n",
    "            bits=BITS, perchannel=True, sym=USE_SYMMETRIC, mse=False\n",
    "        )\n",
    "        gptq_layers[name] = gptq\n",
    "\n",
    "    handles = []\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(module, input, output):\n",
    "            gptq_layers[name].add_batch(input[0].data, output.data)\n",
    "        return hook\n",
    "\n",
    "    for name, layer in layers_to_quantize.items():\n",
    "        handles.append(layer.register_forward_hook(get_activation(name)))\n",
    "\n",
    "    for inputs in dataloader:\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            model(**inputs)\n",
    "\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    print(\"Quantizing layers...\")\n",
    "    for name, layer in layers_to_quantize.items():\n",
    "        print(f\"Quantizing layer {name}\")\n",
    "        gptq = gptq_layers[name]\n",
    "        gptq.fasterquant(\n",
    "            percdamp=PERCENT_DAMPENING,\n",
    "            groupsize=GROUP_SIZE,\n",
    "            actorder=USE_ACT_ORDER,\n",
    "            static_groups=USE_STATIC_GROUPS\n",
    "        )\n",
    "        quantized_weight = gptq.quantizer.quantize(layer.weight.data)\n",
    "        layer.weight.data = quantized_weight.to(dtype)\n",
    "        gptq.free()\n",
    "\n",
    "    print(\"Quantization complete.\")\n",
    "    return model\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load the model\n",
    "model = get_blip2(MODEL_NAME)\n",
    "model.to(DEVICE)\n",
    "# model.eval()\n",
    "\n",
    "# Prepare data loaders\n",
    "dataloader, testloader = get_loaders(\n",
    "    nsamples=NUM_SAMPLES,\n",
    "    seed=SEED,\n",
    "    dataset_path=DATASET_PATH,\n",
    "    annotations_file=ANNOTATIONS_FILE\n",
    ")\n",
    "\n",
    "# Perform quantization if required\n",
    "start_time = time.time()\n",
    "quantized_model = blip2_sequential(model, dataloader)\n",
    "print(f\"Quantization time: {time.time() - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
