{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d424ad7-22fd-45de-a8e6-0b4d5042cc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/miniconda3/envs/LAVIS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting quantization...\n",
      "Quantizing layers...\n",
      "Quantizing layer 0, module self_attn.k_proj\n",
      "Time for quantization: 0.31 seconds\n",
      "Total quantization error: 22786.833984375\n",
      "Quantizing layer 0, module self_attn.v_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 2735.7451171875\n",
      "Quantizing layer 0, module self_attn.q_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 20060.662109375\n",
      "Quantizing layer 0, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 19.52503204345703\n",
      "Quantizing layer 0, module fc1\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 15380.837890625\n",
      "Quantizing layer 0, module fc2\n",
      "Time for quantization: 0.98 seconds\n",
      "Total quantization error: 290.2220764160156\n",
      "Quantizing layer 1, module self_attn.k_proj\n",
      "Time for quantization: 0.25 seconds\n",
      "Total quantization error: 8428.318359375\n",
      "Quantizing layer 1, module self_attn.v_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 892.7083740234375\n",
      "Quantizing layer 1, module self_attn.q_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 10987.689453125\n",
      "Quantizing layer 1, module self_attn.out_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 3.114279270172119\n",
      "Quantizing layer 1, module fc1\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 21110.025390625\n",
      "Quantizing layer 1, module fc2\n",
      "Time for quantization: 0.98 seconds\n",
      "Total quantization error: 267.71075439453125\n",
      "Quantizing layer 2, module self_attn.k_proj\n",
      "Time for quantization: 0.25 seconds\n",
      "Total quantization error: 9934.091796875\n",
      "Quantizing layer 2, module self_attn.v_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 1507.542724609375\n",
      "Quantizing layer 2, module self_attn.q_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 11996.224609375\n",
      "Quantizing layer 2, module self_attn.out_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 12.080514907836914\n",
      "Quantizing layer 2, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 23475.109375\n",
      "Quantizing layer 2, module fc2\n",
      "Time for quantization: 0.96 seconds\n",
      "Total quantization error: 293.064208984375\n",
      "Quantizing layer 3, module self_attn.k_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 13925.509765625\n",
      "Quantizing layer 3, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 2225.99755859375\n",
      "Quantizing layer 3, module self_attn.q_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 16391.515625\n",
      "Quantizing layer 3, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 20.374801635742188\n",
      "Quantizing layer 3, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 26841.244140625\n",
      "Quantizing layer 3, module fc2\n",
      "Time for quantization: 0.97 seconds\n",
      "Total quantization error: 252.6234130859375\n",
      "Quantizing layer 4, module self_attn.k_proj\n",
      "Time for quantization: 0.25 seconds\n",
      "Total quantization error: 29789.26953125\n",
      "Quantizing layer 4, module self_attn.v_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 2938.87109375\n",
      "Quantizing layer 4, module self_attn.q_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 25005.955078125\n",
      "Quantizing layer 4, module self_attn.out_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 42.36650085449219\n",
      "Quantizing layer 4, module fc1\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 31811.90234375\n",
      "Quantizing layer 4, module fc2\n",
      "Time for quantization: 1.04 seconds\n",
      "Total quantization error: 291.6839599609375\n",
      "Quantizing layer 5, module self_attn.k_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 45694.78125\n",
      "Quantizing layer 5, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 4019.20947265625\n",
      "Quantizing layer 5, module self_attn.q_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 32964.90234375\n",
      "Quantizing layer 5, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 42.27786636352539\n",
      "Quantizing layer 5, module fc1\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 38471.0078125\n",
      "Quantizing layer 5, module fc2\n",
      "Time for quantization: 0.96 seconds\n",
      "Total quantization error: 325.78350830078125\n",
      "Quantizing layer 6, module self_attn.k_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 87862.1640625\n",
      "Quantizing layer 6, module self_attn.v_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 5364.20751953125\n",
      "Quantizing layer 6, module self_attn.q_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 38743.27734375\n",
      "Quantizing layer 6, module self_attn.out_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 103.10551452636719\n",
      "Quantizing layer 6, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 47730.8046875\n",
      "Quantizing layer 6, module fc2\n",
      "Time for quantization: 0.96 seconds\n",
      "Total quantization error: 438.798583984375\n",
      "Quantizing layer 7, module self_attn.k_proj\n",
      "Time for quantization: 0.25 seconds\n",
      "Total quantization error: 105064.796875\n",
      "Quantizing layer 7, module self_attn.v_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 6570.0283203125\n",
      "Quantizing layer 7, module self_attn.q_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 40442.91796875\n",
      "Quantizing layer 7, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 138.63027954101562\n",
      "Quantizing layer 7, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 60355.0078125\n",
      "Quantizing layer 7, module fc2\n",
      "Time for quantization: 1.02 seconds\n",
      "Total quantization error: 674.8505859375\n",
      "Quantizing layer 8, module self_attn.k_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 124647.1015625\n",
      "Quantizing layer 8, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 7368.87109375\n",
      "Quantizing layer 8, module self_attn.q_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 43033.6953125\n",
      "Quantizing layer 8, module self_attn.out_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 176.25958251953125\n",
      "Quantizing layer 8, module fc1\n",
      "Time for quantization: 0.25 seconds\n",
      "Total quantization error: 72329.015625\n",
      "Quantizing layer 8, module fc2\n",
      "Time for quantization: 1.05 seconds\n",
      "Total quantization error: 949.9249267578125\n",
      "Quantizing layer 9, module self_attn.k_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 135205.9375\n",
      "Quantizing layer 9, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 8553.6513671875\n",
      "Quantizing layer 9, module self_attn.q_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 43985.0390625\n",
      "Quantizing layer 9, module self_attn.out_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 262.2486267089844\n",
      "Quantizing layer 9, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 82163.578125\n",
      "Quantizing layer 9, module fc2\n",
      "Time for quantization: 1.00 seconds\n",
      "Total quantization error: 1315.05078125\n",
      "Quantizing layer 10, module self_attn.k_proj\n",
      "Time for quantization: 0.26 seconds\n",
      "Total quantization error: 133719.0625\n",
      "Quantizing layer 10, module self_attn.v_proj\n",
      "Time for quantization: 0.25 seconds\n",
      "Total quantization error: 10817.375\n",
      "Quantizing layer 10, module self_attn.q_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 40585.1015625\n",
      "Quantizing layer 10, module self_attn.out_proj\n",
      "Time for quantization: 0.25 seconds\n",
      "Total quantization error: 384.5782470703125\n",
      "Quantizing layer 10, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 93244.0\n",
      "Quantizing layer 10, module fc2\n",
      "Time for quantization: 1.00 seconds\n",
      "Total quantization error: 2235.47021484375\n",
      "Quantizing layer 11, module self_attn.k_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 130864.140625\n",
      "Quantizing layer 11, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 13473.541015625\n",
      "Quantizing layer 11, module self_attn.q_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 40154.515625\n",
      "Quantizing layer 11, module self_attn.out_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 580.4441528320312\n",
      "Quantizing layer 11, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 103395.78125\n",
      "Quantizing layer 11, module fc2\n",
      "Time for quantization: 1.01 seconds\n",
      "Total quantization error: 3157.898193359375\n",
      "Quantizing layer 12, module self_attn.k_proj\n",
      "Time for quantization: 0.26 seconds\n",
      "Total quantization error: 165185.21875\n",
      "Quantizing layer 12, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 15036.94921875\n",
      "Quantizing layer 12, module self_attn.q_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 42218.2734375\n",
      "Quantizing layer 12, module self_attn.out_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 860.5955810546875\n",
      "Quantizing layer 12, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 110089.1171875\n",
      "Quantizing layer 12, module fc2\n",
      "Time for quantization: 1.02 seconds\n",
      "Total quantization error: 3952.010498046875\n",
      "Quantizing layer 13, module self_attn.k_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 197353.359375\n",
      "Quantizing layer 13, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 17345.96875\n",
      "Quantizing layer 13, module self_attn.q_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 44826.1171875\n",
      "Quantizing layer 13, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 1177.990234375\n",
      "Quantizing layer 13, module fc1\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 125841.2734375\n",
      "Quantizing layer 13, module fc2\n",
      "Time for quantization: 1.01 seconds\n",
      "Total quantization error: 5259.283203125\n",
      "Quantizing layer 14, module self_attn.k_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 172582.875\n",
      "Quantizing layer 14, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 23080.41796875\n",
      "Quantizing layer 14, module self_attn.q_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 43414.9609375\n",
      "Quantizing layer 14, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 1373.3466796875\n",
      "Quantizing layer 14, module fc1\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 144665.03125\n",
      "Quantizing layer 14, module fc2\n",
      "Time for quantization: 0.95 seconds\n",
      "Total quantization error: 8204.015625\n",
      "Quantizing layer 15, module self_attn.k_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 140242.15625\n",
      "Quantizing layer 15, module self_attn.v_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 30712.65625\n",
      "Quantizing layer 15, module self_attn.q_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 45534.0\n",
      "Quantizing layer 15, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 2031.1083984375\n",
      "Quantizing layer 15, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 160903.25\n",
      "Quantizing layer 15, module fc2\n",
      "Time for quantization: 0.96 seconds\n",
      "Total quantization error: 10451.7802734375\n",
      "Quantizing layer 16, module self_attn.k_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 167449.359375\n",
      "Quantizing layer 16, module self_attn.v_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 34023.3671875\n",
      "Quantizing layer 16, module self_attn.q_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 42468.06640625\n",
      "Quantizing layer 16, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 2746.60498046875\n",
      "Quantizing layer 16, module fc1\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 180145.84375\n",
      "Quantizing layer 16, module fc2\n",
      "Time for quantization: 0.95 seconds\n",
      "Total quantization error: 13218.298828125\n",
      "Quantizing layer 17, module self_attn.k_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 153823.515625\n",
      "Quantizing layer 17, module self_attn.v_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 37860.078125\n",
      "Quantizing layer 17, module self_attn.q_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 44882.96875\n",
      "Quantizing layer 17, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 2657.378173828125\n",
      "Quantizing layer 17, module fc1\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 199218.71875\n",
      "Quantizing layer 17, module fc2\n",
      "Time for quantization: 0.97 seconds\n",
      "Total quantization error: 16266.0146484375\n",
      "Quantizing layer 18, module self_attn.k_proj\n",
      "Time for quantization: 0.25 seconds\n",
      "Total quantization error: 117608.5546875\n",
      "Quantizing layer 18, module self_attn.v_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 43127.1953125\n",
      "Quantizing layer 18, module self_attn.q_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 43524.09375\n",
      "Quantizing layer 18, module self_attn.out_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 3537.995361328125\n",
      "Quantizing layer 18, module fc1\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 211738.0\n",
      "Quantizing layer 18, module fc2\n",
      "Time for quantization: 1.01 seconds\n",
      "Total quantization error: 19349.806640625\n",
      "Quantizing layer 19, module self_attn.k_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 112858.984375\n",
      "Quantizing layer 19, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 55198.953125\n",
      "Quantizing layer 19, module self_attn.q_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 43469.9296875\n",
      "Quantizing layer 19, module self_attn.out_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 4919.98388671875\n",
      "Quantizing layer 19, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 230822.1875\n",
      "Quantizing layer 19, module fc2\n",
      "Time for quantization: 0.96 seconds\n",
      "Total quantization error: 23293.859375\n",
      "Quantizing layer 20, module self_attn.k_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 120578.5\n",
      "Quantizing layer 20, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 66011.78125\n",
      "Quantizing layer 20, module self_attn.q_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 44251.82421875\n",
      "Quantizing layer 20, module self_attn.out_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 6303.21337890625\n",
      "Quantizing layer 20, module fc1\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 244782.65625\n",
      "Quantizing layer 20, module fc2\n",
      "Time for quantization: 0.97 seconds\n",
      "Total quantization error: 28552.978515625\n",
      "Quantizing layer 21, module self_attn.k_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 90071.46875\n",
      "Quantizing layer 21, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 70126.125\n",
      "Quantizing layer 21, module self_attn.q_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 48309.53125\n",
      "Quantizing layer 21, module self_attn.out_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 5168.7138671875\n",
      "Quantizing layer 21, module fc1\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 248634.359375\n",
      "Quantizing layer 21, module fc2\n",
      "Time for quantization: 0.99 seconds\n",
      "Total quantization error: 37856.5625\n",
      "Quantizing layer 22, module self_attn.k_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 75861.9765625\n",
      "Quantizing layer 22, module self_attn.v_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 78165.71875\n",
      "Quantizing layer 22, module self_attn.q_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 58195.1640625\n",
      "Quantizing layer 22, module self_attn.out_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 4317.185546875\n",
      "Quantizing layer 22, module fc1\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 239927.25\n",
      "Quantizing layer 22, module fc2\n",
      "Time for quantization: 1.02 seconds\n",
      "Total quantization error: 41682.03515625\n",
      "Quantizing layer 23, module self_attn.k_proj\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 96399.34375\n",
      "Quantizing layer 23, module self_attn.v_proj\n",
      "Time for quantization: 0.22 seconds\n",
      "Total quantization error: 58770.75\n",
      "Quantizing layer 23, module self_attn.q_proj\n",
      "Time for quantization: 0.21 seconds\n",
      "Total quantization error: 100034.984375\n",
      "Quantizing layer 23, module self_attn.out_proj\n",
      "Time for quantization: 0.23 seconds\n",
      "Total quantization error: 5149.978515625\n",
      "Quantizing layer 23, module fc1\n",
      "Time for quantization: 0.24 seconds\n",
      "Total quantization error: 231690.875\n",
      "Quantizing layer 23, module fc2\n",
      "Time for quantization: 0.95 seconds\n",
      "Total quantization error: 36454.9765625\n",
      "Quantization complete.\n",
      "Quantization time: 154.36 seconds\n",
      "Evaluating on wikitext2 dataset...\n",
      "Evaluating...\n",
      "Perplexity: 15.74\n",
      "Perplexity on wikitext2: 15.74\n",
      "Evaluating on ptb-new dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/miniconda3/envs/LAVIS/lib/python3.8/site-packages/datasets/load.py:1486: FutureWarning: The repository for ptb_text_only contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ptb_text_only\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Perplexity: 22.39\n",
      "Perplexity on ptb-new: 22.39\n",
      "Quantized model saved.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ====================================================\n",
    "# Constants and Configuration\n",
    "# ====================================================\n",
    "\n",
    "# Model and dataset configuration\n",
    "MODEL_NAME = \"facebook/opt-1.3b\"  # Name of the OPT model to load\n",
    "DATASET_NAME = \"wikitext2\"  # Calibration dataset (\"wikitext2\" or \"ptb-new\")\n",
    "\n",
    "# Quantization parameters\n",
    "SEED = 0  # Random seed for reproducibility\n",
    "NUM_SAMPLES = 128  # Number of calibration samples\n",
    "PERCENT_DAMPENING = 0.01  # Dampening percentage during quantization\n",
    "BITS = 4  # Number of bits for quantization\n",
    "GROUP_SIZE = -1  # Group size for quantization (-1 means no grouping)\n",
    "USE_SYMMETRIC = True  # Use symmetric quantization\n",
    "USE_ACT_ORDER = False  # Use activation order during quantization\n",
    "USE_STATIC_GROUPS = False  # Use static groups during quantization\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Debugging flag\n",
    "DEBUG = False  # Set to True for debugging output\n",
    "\n",
    "# Disable TensorFloat32 for matmul and cuDNN to ensure deterministic results\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# ====================================================\n",
    "# Quantization Functions and Classes\n",
    "# ====================================================\n",
    "\n",
    "def quantize_tensor(tensor, scale, zero_point, max_quant):\n",
    "    \"\"\"\n",
    "    Quantize the input tensor using the provided scale and zero point.\n",
    "    \"\"\"\n",
    "    if max_quant < 0:\n",
    "        # Special case for ternary quantization\n",
    "        return (tensor > scale / 2).float() * scale + (tensor < zero_point / 2).float() * zero_point\n",
    "    quantized = torch.clamp(torch.round(tensor / scale) + zero_point, 0, max_quant)\n",
    "    return scale * (quantized - zero_point)\n",
    "\n",
    "class Quantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantizer class to handle quantization parameters and operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, shape=1):\n",
    "        super(Quantizer, self).__init__()\n",
    "        self.register_buffer('max_quant', torch.tensor(0))  # Maximum quantization level\n",
    "        self.register_buffer('scale', torch.zeros(shape))  # Scale for quantization\n",
    "        self.register_buffer('zero_point', torch.zeros(shape))  # Zero point for quantization\n",
    "\n",
    "    def configure(self, bits, per_channel=False, symmetric=True, use_mse=False,\n",
    "                  error_norm=2.4, grid_size=100, max_shrink=0.8, use_ternary=False):\n",
    "        \"\"\"\n",
    "        Configure the quantizer with the specified parameters.\n",
    "        \"\"\"\n",
    "        self.max_quant = torch.tensor(2 ** bits - 1)\n",
    "        self.per_channel = per_channel  # Whether to quantize per channel\n",
    "        self.symmetric = symmetric  # Symmetric quantization\n",
    "        self.use_mse = use_mse  # Use MSE to find optimal scale and zero point\n",
    "        self.error_norm = error_norm  # Norm to compute quantization error\n",
    "        self.grid_size = grid_size  # Grid size for scale search\n",
    "        self.max_shrink = max_shrink  # Maximum shrinkage of quantization range during search\n",
    "        if use_ternary:\n",
    "            self.max_quant = torch.tensor(-1)  # Special value for ternary quantization\n",
    "\n",
    "    def find_params(self, tensor, is_weight=False):\n",
    "        \"\"\"\n",
    "        Find the scale and zero point parameters for quantization based on the input tensor.\n",
    "        \"\"\"\n",
    "        device = tensor.device\n",
    "        self.max_quant = self.max_quant.to(device)\n",
    "\n",
    "        shape = tensor.shape\n",
    "        if self.per_channel:\n",
    "            # Per-channel quantization\n",
    "            if is_weight:\n",
    "                tensor = tensor.flatten(1)\n",
    "            else:\n",
    "                if len(shape) == 4:\n",
    "                    tensor = tensor.permute(1, 0, 2, 3).flatten(1)\n",
    "                elif len(shape) == 3:\n",
    "                    tensor = tensor.reshape(-1, shape[-1]).t()\n",
    "                elif len(shape) == 2:\n",
    "                    tensor = tensor.t()\n",
    "        else:\n",
    "            # Global quantization\n",
    "            tensor = tensor.flatten().unsqueeze(0)\n",
    "\n",
    "        zeros = torch.zeros(tensor.shape[0], device=device)\n",
    "        tensor_min = torch.minimum(tensor.min(dim=1)[0], zeros)\n",
    "        tensor_max = torch.maximum(tensor.max(dim=1)[0], zeros)\n",
    "\n",
    "        if self.symmetric:\n",
    "            tensor_max = torch.maximum(torch.abs(tensor_min), tensor_max)\n",
    "            negative_mask = tensor_min < 0\n",
    "            if torch.any(negative_mask):\n",
    "                tensor_min[negative_mask] = -tensor_max[negative_mask]\n",
    "        zero_mask = (tensor_min == 0) & (tensor_max == 0)\n",
    "        tensor_min[zero_mask] = -1\n",
    "        tensor_max[zero_mask] = 1\n",
    "\n",
    "        if self.max_quant < 0:\n",
    "            # Special case for ternary quantization\n",
    "            self.scale = tensor_max\n",
    "            self.zero_point = tensor_min\n",
    "        else:\n",
    "            self.scale = (tensor_max - tensor_min) / self.max_quant\n",
    "            if self.symmetric:\n",
    "                self.zero_point = torch.full_like(self.scale, (self.max_quant + 1) / 2)\n",
    "            else:\n",
    "                self.zero_point = torch.round(-tensor_min / self.scale)\n",
    "\n",
    "        if self.use_mse:\n",
    "            # Use Mean Squared Error to find optimal scale and zero point\n",
    "            best_error = torch.full([tensor.shape[0]], float('inf'), device=device)\n",
    "            for i in range(int(self.max_shrink * self.grid_size)):\n",
    "                p = 1 - i / self.grid_size\n",
    "                tensor_min1 = p * tensor_min\n",
    "                tensor_max1 = p * tensor_max\n",
    "                scale1 = (tensor_max1 - tensor_min1) / self.max_quant\n",
    "                if not self.symmetric:\n",
    "                    zero_point1 = torch.round(-tensor_min1 / scale1)\n",
    "                else:\n",
    "                    zero_point1 = self.zero_point\n",
    "                q = quantize_tensor(tensor, scale1.unsqueeze(1), zero_point1.unsqueeze(1), self.max_quant)\n",
    "                error = ((q - tensor).abs().pow(self.error_norm)).sum(dim=1)\n",
    "                better_error_mask = error < best_error\n",
    "                if torch.any(better_error_mask):\n",
    "                    best_error[better_error_mask] = error[better_error_mask]\n",
    "                    self.scale[better_error_mask] = scale1[better_error_mask]\n",
    "                    self.zero_point[better_error_mask] = zero_point1[better_error_mask]\n",
    "\n",
    "        if not self.per_channel:\n",
    "            repeat_times = shape[0] if is_weight else shape[1] if len(shape) != 3 else shape[2]\n",
    "            self.scale = self.scale.repeat(repeat_times)\n",
    "            self.zero_point = self.zero_point.repeat(repeat_times)\n",
    "\n",
    "        if is_weight:\n",
    "            # Reshape for weight tensors\n",
    "            new_shape = [-1] + [1] * (len(shape) - 1)\n",
    "            self.scale = self.scale.reshape(new_shape)\n",
    "            self.zero_point = self.zero_point.reshape(new_shape)\n",
    "            return\n",
    "\n",
    "        # Reshape for activation tensors\n",
    "        if len(shape) == 4:\n",
    "            self.scale = self.scale.reshape(1, -1, 1, 1)\n",
    "            self.zero_point = self.zero_point.reshape(1, -1, 1, 1)\n",
    "        elif len(shape) == 3:\n",
    "            self.scale = self.scale.reshape(1, 1, -1)\n",
    "            self.zero_point = self.zero_point.reshape(1, 1, -1)\n",
    "        elif len(shape) == 2:\n",
    "            self.scale = self.scale.unsqueeze(0)\n",
    "            self.zero_point = self.zero_point.unsqueeze(0)\n",
    "\n",
    "    def quantize(self, tensor):\n",
    "        \"\"\"\n",
    "        Quantize the input tensor using the stored scale and zero point.\n",
    "        \"\"\"\n",
    "        if self.ready():\n",
    "            return quantize_tensor(tensor, self.scale, self.zero_point, self.max_quant)\n",
    "        return tensor\n",
    "\n",
    "    def enabled(self):\n",
    "        \"\"\"\n",
    "        Check if quantization is enabled (max_quant > 0).\n",
    "        \"\"\"\n",
    "        return self.max_quant > 0\n",
    "\n",
    "    def ready(self):\n",
    "        \"\"\"\n",
    "        Check if the quantizer is ready (scale is non-zero).\n",
    "        \"\"\"\n",
    "        return torch.all(self.scale != 0)\n",
    "\n",
    "# ====================================================\n",
    "# GPTQ Quantization Class\n",
    "# ====================================================\n",
    "\n",
    "class GPTQuantizer:\n",
    "    \"\"\"\n",
    "    GPTQuantizer class to perform quantization of a given model layer using GPTQ algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.device = self.layer.weight.device\n",
    "        weight_data = layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            weight_data = weight_data.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            weight_data = weight_data.t()\n",
    "        self.num_rows, self.num_columns = weight_data.shape\n",
    "        self.hessian_inv = torch.zeros((self.num_columns, self.num_columns), device=self.device)\n",
    "        self.num_samples = 0  # Number of samples collected\n",
    "        self.quantizer = Quantizer()\n",
    "\n",
    "    def add_batch(self, inp, out):\n",
    "        \"\"\"\n",
    "        Add a batch of input and output data to compute the Hessian approximation.\n",
    "        \"\"\"\n",
    "        if DEBUG:\n",
    "            self.debug_input = inp\n",
    "            self.debug_output = out\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        batch_size = inp.shape[0]\n",
    "        if isinstance(self.layer, (nn.Linear, transformers.Conv1D)):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape(-1, inp.shape[-1])\n",
    "            inp = inp.t()\n",
    "        elif isinstance(self.layer, nn.Conv2d):\n",
    "            unfold = nn.Unfold(\n",
    "                kernel_size=self.layer.kernel_size,\n",
    "                dilation=self.layer.dilation,\n",
    "                padding=self.layer.padding,\n",
    "                stride=self.layer.stride\n",
    "            )\n",
    "            inp = unfold(inp)\n",
    "            inp = inp.permute(1, 0, 2).flatten(1)\n",
    "        # Update Hessian approximation\n",
    "        self.hessian_inv *= self.num_samples / (self.num_samples + batch_size)\n",
    "        self.num_samples += batch_size\n",
    "        inp = math.sqrt(2 / self.num_samples) * inp.float()\n",
    "        self.hessian_inv += inp @ inp.t()\n",
    "\n",
    "    def perform_quantization(self, block_size=128, percent_dampening=0.01, group_size=-1,\n",
    "                             use_activation_order=False, use_static_groups=False):\n",
    "        \"\"\"\n",
    "        Perform quantization using the collected data and Hessian approximation.\n",
    "        \"\"\"\n",
    "        weight_data = self.layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            weight_data = weight_data.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            weight_data = weight_data.t()\n",
    "        weight_data = weight_data.float()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not self.quantizer.ready():\n",
    "            self.quantizer.find_params(weight_data, is_weight=True)\n",
    "\n",
    "        H = self.hessian_inv\n",
    "        del self.hessian_inv\n",
    "        zero_diagonal = torch.diag(H) == 0\n",
    "        H[zero_diagonal, zero_diagonal] = 1\n",
    "        weight_data[:, zero_diagonal] = 0\n",
    "\n",
    "        if use_static_groups:\n",
    "            import copy\n",
    "            groups = []\n",
    "            for i in range(0, self.num_columns, group_size):\n",
    "                quantizer_copy = copy.deepcopy(self.quantizer)\n",
    "                quantizer_copy.find_params(weight_data[:, i:i+group_size], is_weight=True)\n",
    "                groups.append(quantizer_copy)\n",
    "\n",
    "        if use_activation_order:\n",
    "            perm = torch.argsort(torch.diag(H), descending=True)\n",
    "            weight_data = weight_data[:, perm]\n",
    "            H = H[perm][:, perm]\n",
    "            inv_perm = torch.argsort(perm)\n",
    "\n",
    "        losses = torch.zeros_like(weight_data)\n",
    "        quantized_weight = torch.zeros_like(weight_data)\n",
    "\n",
    "        dampening = percent_dampening * torch.mean(torch.diag(H))\n",
    "        diag_indices = torch.arange(self.num_columns, device=self.device)\n",
    "        H[diag_indices, diag_indices] += dampening\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        H_inv = H\n",
    "\n",
    "        for start_idx in range(0, self.num_columns, block_size):\n",
    "            end_idx = min(start_idx + block_size, self.num_columns)\n",
    "            block_count = end_idx - start_idx\n",
    "\n",
    "            W_block = weight_data[:, start_idx:end_idx].clone()\n",
    "            Q_block = torch.zeros_like(W_block)\n",
    "            Error_block = torch.zeros_like(W_block)\n",
    "            Losses_block = torch.zeros_like(W_block)\n",
    "            H_inv_block = H_inv[start_idx:end_idx, start_idx:end_idx]\n",
    "\n",
    "            for i in range(block_count):\n",
    "                w = W_block[:, i]\n",
    "                d = H_inv_block[i, i]\n",
    "\n",
    "                if group_size != -1:\n",
    "                    if not use_static_groups:\n",
    "                        if (start_idx + i) % group_size == 0:\n",
    "                            self.quantizer.find_params(weight_data[:, (start_idx + i):(start_idx + i + group_size)], is_weight=True)\n",
    "                    else:\n",
    "                        idx = start_idx + i\n",
    "                        if use_activation_order:\n",
    "                            idx = perm[idx]\n",
    "                        self.quantizer = groups[idx // group_size]\n",
    "\n",
    "                q = quantize_tensor(w.unsqueeze(1), self.quantizer.scale, self.quantizer.zero_point, self.quantizer.max_quant).flatten()\n",
    "                Q_block[:, i] = q\n",
    "                Losses_block[:, i] = ((w - q) ** 2) / (d ** 2) / 2\n",
    "\n",
    "                err = (w - q) / d\n",
    "                W_block[:, i:] -= err.unsqueeze(1) @ H_inv_block[i, i:].unsqueeze(0)\n",
    "                Error_block[:, i] = err\n",
    "\n",
    "            quantized_weight[:, start_idx:end_idx] = Q_block\n",
    "            losses[:, start_idx:end_idx] = Losses_block\n",
    "\n",
    "            weight_data[:, end_idx:] -= Error_block @ H_inv[start_idx:end_idx, end_idx:]\n",
    "\n",
    "            if DEBUG:\n",
    "                self.layer.weight.data[:, :end_idx] = quantized_weight[:, :end_idx]\n",
    "                self.layer.weight.data[:, end_idx:] = weight_data[:, end_idx:]\n",
    "                print(torch.sum((self.layer(self.debug_input) - self.debug_output) ** 2))\n",
    "                print(torch.sum(losses))\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Time for quantization: {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"Total quantization error: {torch.sum(losses).item()}\")\n",
    "\n",
    "        if use_activation_order:\n",
    "            quantized_weight = quantized_weight[:, inv_perm]\n",
    "\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            quantized_weight = quantized_weight.t()\n",
    "        self.layer.weight.data = quantized_weight.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)\n",
    "        if DEBUG:\n",
    "            print(torch.sum((self.layer(self.debug_input) - self.debug_output) ** 2))\n",
    "\n",
    "    def free(self):\n",
    "        \"\"\"\n",
    "        Free up memory by deleting large variables.\n",
    "        \"\"\"\n",
    "        if DEBUG:\n",
    "            self.debug_input = None\n",
    "            self.debug_output = None\n",
    "        self.hessian_inv = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ====================================================\n",
    "# Data Loader Functions\n",
    "# ====================================================\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def load_wikitext2(nsamples, seed, sequence_length, model_name):\n",
    "    \"\"\"\n",
    "    Load the WikiText-2 dataset and prepare calibration and test data loaders.\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    # Load the dataset\n",
    "    train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the data\n",
    "    train_encodings = tokenizer(\"\\n\\n\".join(train_dataset[\"text\"]), return_tensors=\"pt\")\n",
    "    test_encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    # Set random seed\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    # Prepare the calibration data loader\n",
    "    calibration_data = []\n",
    "    for _ in range(nsamples):\n",
    "        start_idx = random.randint(0, train_encodings.input_ids.shape[1] - sequence_length - 1)\n",
    "        end_idx = start_idx + sequence_length\n",
    "        input_ids = train_encodings.input_ids[:, start_idx:end_idx]\n",
    "        labels = input_ids.clone()\n",
    "        labels[:, :-1] = -100  # Mask for language modeling loss\n",
    "        calibration_data.append((input_ids, labels))\n",
    "    return calibration_data, test_encodings\n",
    "\n",
    "def load_ptb_new(nsamples, seed, sequence_length, model_name):\n",
    "    \"\"\"\n",
    "    Load the Penn Treebank (PTB) dataset and prepare calibration and test data loaders.\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    # Load the dataset\n",
    "    train_dataset = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"train\")\n",
    "    test_dataset = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\")\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize the data\n",
    "    train_encodings = tokenizer(\" \".join(train_dataset[\"sentence\"]), return_tensors=\"pt\")\n",
    "    test_encodings = tokenizer(\" \".join(test_dataset[\"sentence\"]), return_tensors=\"pt\")\n",
    "\n",
    "    # Set random seed\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    # Prepare the calibration data loader\n",
    "    calibration_data = []\n",
    "    for _ in range(nsamples):\n",
    "        start_idx = random.randint(0, train_encodings.input_ids.shape[1] - sequence_length - 1)\n",
    "        end_idx = start_idx + sequence_length\n",
    "        input_ids = train_encodings.input_ids[:, start_idx:end_idx]\n",
    "        labels = input_ids.clone()\n",
    "        labels[:, :-1] = -100  # Mask for language modeling loss\n",
    "        calibration_data.append((input_ids, labels))\n",
    "    return calibration_data, test_encodings\n",
    "\n",
    "def get_data_loaders(dataset_name, nsamples, seed, sequence_length, model_name):\n",
    "    \"\"\"\n",
    "    Get the calibration and test data loaders for the specified dataset.\n",
    "    \"\"\"\n",
    "    if dataset_name == \"wikitext2\":\n",
    "        return load_wikitext2(nsamples, seed, sequence_length, model_name)\n",
    "    elif dataset_name == \"ptb-new\":\n",
    "        return load_ptb_new(nsamples, seed, sequence_length, model_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported.\")\n",
    "\n",
    "# ====================================================\n",
    "# Model Utility Functions\n",
    "# ====================================================\n",
    "\n",
    "def find_target_layers(module, target_layers=[nn.Conv2d, nn.Linear, transformers.Conv1D], name=''):\n",
    "    \"\"\"\n",
    "    Recursively find all layers of specified types in a model.\n",
    "    Returns a dictionary mapping layer names to layers.\n",
    "    \"\"\"\n",
    "    if type(module) in target_layers:\n",
    "        return {name: module}\n",
    "    found_layers = {}\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_layers = find_target_layers(\n",
    "            child_module, target_layers=target_layers,\n",
    "            name=f\"{name}.{child_name}\" if name else child_name\n",
    "        )\n",
    "        found_layers.update(child_layers)\n",
    "    return found_layers\n",
    "\n",
    "def load_opt_model(model_name):\n",
    "    \"\"\"\n",
    "    Load and prepare the OPT model for quantization.\n",
    "    \"\"\"\n",
    "    # Disable weight initialization to speed up model loading\n",
    "    def skip_weight_init(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    torch.nn.init.kaiming_uniform_ = skip_weight_init\n",
    "    torch.nn.init.uniform_ = skip_weight_init\n",
    "    torch.nn.init.normal_ = skip_weight_init\n",
    "\n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto')\n",
    "    model.sequence_length = model.config.max_position_embeddings\n",
    "    return model\n",
    "\n",
    "# ====================================================\n",
    "# Quantization and Evaluation Functions\n",
    "# ====================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_model_sequentially(model, calibration_data, device):\n",
    "    \"\"\"\n",
    "    Perform quantization on the OPT model layers sequentially.\n",
    "    The quantized model is modified in-place.\n",
    "    \"\"\"\n",
    "    print(\"Starting quantization...\")\n",
    "\n",
    "    # Disable cache to prevent unexpected behaviors\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    layers = model.model.decoder.layers\n",
    "\n",
    "    # Move embedding layers to the specified device\n",
    "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(device)\n",
    "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(device)\n",
    "    \n",
    "    # Prepare for quantization\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inputs = torch.zeros((NUM_SAMPLES, model.sequence_length, model.config.hidden_size), dtype=dtype, device=device)\n",
    "    cache = {\"index\": 0, \"attention_mask\": None}\n",
    "    \n",
    "    # Define a module to capture intermediate inputs\n",
    "    class InputRecorder(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inputs[cache[\"index\"]] = inp\n",
    "            cache[\"index\"] += 1\n",
    "            cache[\"attention_mask\"] = kwargs.get(\"attention_mask\")\n",
    "            raise ValueError  # Interrupt after capturing input\n",
    "\n",
    "    # Replace the first layer with the recorder to collect inputs\n",
    "    layers[0] = InputRecorder(layers[0])\n",
    "    for batch, _ in calibration_data:\n",
    "        try:\n",
    "            batch = batch.to(device)\n",
    "            model(batch)\n",
    "        except ValueError:\n",
    "            pass  # Catch the exception to stop after capturing input\n",
    "    layers[0] = layers[0].module  # Restore the original layer\n",
    "\n",
    "    # Move layers back to CPU to free up GPU memory\n",
    "    layers[0] = layers[0].cpu()\n",
    "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.cpu()\n",
    "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    outputs = torch.zeros_like(inputs)\n",
    "    attention_mask = cache[\"attention_mask\"]\n",
    "\n",
    "    print(\"Quantizing layers...\")\n",
    "    for layer_index in range(len(layers)):\n",
    "        layer = layers[layer_index].to(device)\n",
    "        \n",
    "        # Prepare for quantization\n",
    "        target_layers = find_target_layers(layer)\n",
    "        quantizers = {}\n",
    "        for name in target_layers:\n",
    "            quantizers[name] = GPTQuantizer(target_layers[name])\n",
    "            quantizers[name].quantizer.configure(\n",
    "                bits=BITS, per_channel=True, symmetric=USE_SYMMETRIC, use_mse=False\n",
    "            )\n",
    "\n",
    "        # Define a hook to collect data for quantization\n",
    "        def collect_activations(name):\n",
    "            def hook(module, inp, out):\n",
    "                quantizers[name].add_batch(inp[0].data, out.data)\n",
    "            return hook\n",
    "\n",
    "        # Register hooks\n",
    "        hooks = []\n",
    "        for name in target_layers:\n",
    "            hooks.append(target_layers[name].register_forward_hook(collect_activations(name)))\n",
    "\n",
    "        # Run the model to collect data\n",
    "        for sample_index in range(NUM_SAMPLES):\n",
    "            outputs[sample_index] = layer(inputs[sample_index].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "\n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # Perform quantization\n",
    "        for name in target_layers:\n",
    "            print(f\"Quantizing layer {layer_index}, module {name}\")\n",
    "            quantizers[name].perform_quantization(\n",
    "                percent_dampening=PERCENT_DAMPENING,\n",
    "                group_size=GROUP_SIZE,\n",
    "                use_activation_order=USE_ACT_ORDER,\n",
    "                use_static_groups=USE_STATIC_GROUPS\n",
    "            )\n",
    "            # Apply quantization to the layer\n",
    "            quantized_weight = quantizers[name].quantizer.quantize(target_layers[name].weight.data)\n",
    "            target_layers[name].weight.data = quantized_weight.to(dtype)\n",
    "            quantizers[name].free()\n",
    "\n",
    "        # Update inputs for the next layer\n",
    "        for sample_index in range(NUM_SAMPLES):\n",
    "            outputs[sample_index] = layer(inputs[sample_index].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "\n",
    "        # Move the quantized layer back to CPU\n",
    "        layers[layer_index] = layer.cpu()\n",
    "        del layer\n",
    "        del quantizers\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Swap inputs and outputs for the next iteration\n",
    "        inputs, outputs = outputs, inputs\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "    print(\"Quantization complete.\")\n",
    "    return model  # Return the quantized model\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, test_encodings, device):\n",
    "    \"\"\"\n",
    "    Evaluate the quantized OPT model and calculate perplexity.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating...\")\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    input_ids = test_encodings.input_ids\n",
    "    total_tokens = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(0, input_ids.size(1), model.sequence_length):\n",
    "        batch = input_ids[:, i:i+model.sequence_length].to(device)\n",
    "        if batch.size(1) < 2:\n",
    "            continue  # Skip batches that are too small\n",
    "        outputs = model(batch)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = batch[:, 1:].contiguous()\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='sum')\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += (shift_labels != -100).sum().item()\n",
    "\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\n",
    "    print(f\"Perplexity: {perplexity.item():.2f}\")\n",
    "    return perplexity.item()\n",
    "\n",
    "# ====================================================\n",
    "# Main Execution\n",
    "# ====================================================\n",
    "\n",
    "# Set random seed\n",
    "set_random_seed(SEED)\n",
    "\n",
    "# Load the model\n",
    "model = load_opt_model(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "# Prepare data loaders\n",
    "calibration_data, test_encodings = get_data_loaders(\n",
    "    DATASET_NAME,\n",
    "    nsamples=NUM_SAMPLES,\n",
    "    seed=SEED,\n",
    "    sequence_length=model.sequence_length,\n",
    "    model_name=MODEL_NAME,\n",
    ")\n",
    "\n",
    "# Perform quantization if required\n",
    "if BITS < 16:\n",
    "    start_time = time.time()\n",
    "    model = quantize_model_sequentially(model, calibration_data, DEVICE)\n",
    "    print(f\"Quantization time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the model\n",
    "datasets = [\"wikitext2\", \"ptb-new\"]\n",
    "for dataset in datasets:\n",
    "    print(f\"Evaluating on {dataset} dataset...\")\n",
    "    _, test_encodings = get_data_loaders(\n",
    "        dataset, nsamples=NUM_SAMPLES, seed=SEED, sequence_length=model.sequence_length, model_name=MODEL_NAME\n",
    "    )\n",
    "    perplexity = evaluate_model(model, test_encodings, DEVICE)\n",
    "    print(f\"Perplexity on {dataset}: {perplexity:.2f}\")\n",
    "\n",
    "# Save the quantized model\n",
    "model.save_pretrained(\"quantized_opt_model\")\n",
    "print(\"Quantized model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4789e2-170d-4601-be7d-b9c7e161cd95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
